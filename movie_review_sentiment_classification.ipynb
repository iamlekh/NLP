{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie_review_sentiment_classification",
      "provenance": [],
      "mount_file_id": "1NXibxq8HU_yttbrDr4X_0V1pIbCPiHe3",
      "authorship_tag": "ABX9TyNIOQX5kuEdZHuBtv4x5yev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamlekh/NLP/blob/master/movie_review_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XvmJE5g-7Td",
        "colab_type": "text"
      },
      "source": [
        "## **Load Text Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIX2pLYm7zB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/review_polarity/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoYN3gYvAR3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from pandas import DataFrame\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPc5vOpM9Ggg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  file = open(filename, 'r' )\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "\n",
        "def process_docs(directory):\n",
        "  for filename in listdir(directory):\n",
        "    if not filename.endswith(\".txt\"):\n",
        "      next\n",
        "    path = directory + '/' + filename\n",
        "    doc = load_doc(path)\n",
        "    print( 'Loaded %s' % filename)\n",
        "\n",
        "# directory = path + '/txt_sentoken/neg'\n",
        "# process_docs(directory)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbUEd_Uh_PWi",
        "colab_type": "text"
      },
      "source": [
        "# **Clean Text Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD6xQ8Xi-1kr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "  tokens = doc.split()\n",
        "  re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n",
        "  tokens = [re_punc.sub( '' , w) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  stop_words = set(stopwords.words( 'english' ))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuxN11ONA1Sv",
        "colab_type": "text"
      },
      "source": [
        "## **Vocab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1Vn41i5-1qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_doc_to_vocab(filename, vocab):\n",
        "  doc = load_doc(filename)\n",
        "  tokens = clean_doc(doc)\n",
        "  vocab.update(tokens)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOIDf8Fy9HSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_docs(directory, vocab):\n",
        "  for filename in listdir(directory):\n",
        "    if not filename.endswith(\".txt\"):\n",
        "      next\n",
        "    path = directory + '/' + filename\n",
        "    add_doc_to_vocab(path, vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yec40f1v9HU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "  data = '\\n' .join(lines)\n",
        "  file = open(filename, 'w' )\n",
        "  file.write(data)\n",
        "  file.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkcDF5kNBkqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1079e0c5-3691-4cf3-d833-7d331af078bf"
      },
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "\n",
        "process_docs( path + 'txt_sentoken/neg' , vocab)\n",
        "process_docs( path + 'txt_sentoken/pos' , vocab)\n",
        "\n",
        "print(len(vocab))\n",
        "\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "min_occurane = 5\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "\n",
        "save_list(tokens, path + 'vocab.txt' )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46557\n",
            "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
            "14803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwestCODCXBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m nltk.downloader all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmozatNMG3PC",
        "colab_type": "text"
      },
      "source": [
        "## **Bag-of-Words Representation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D2wmsLnCoFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "  doc = load_doc(filename)\n",
        "  tokens = clean_doc(doc)\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  return ' ' .join(tokens)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf4OwIYaHlAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  lines = list()\n",
        "  for filename in listdir(directory):\n",
        "    if is_train and filename.startswith( 'cv9' ):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith( 'cv9' ):\n",
        "      continue\n",
        "    path = directory + '/' + filename\n",
        "    line = doc_to_line(path, vocab)\n",
        "    lines.append(line)\n",
        "  return lines"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB60WsiTIoQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "  neg = process_docs( path + 'txt_sentoken/neg' , vocab, is_train)\n",
        "  pos = process_docs( path + 'txt_sentoken/pos' , vocab, is_train)\n",
        "  docs = neg + pos\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcH6GAbqKeD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klh6N7fvIocX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # load the vocabulary\n",
        "# vocab_filename = path + 'vocab.txt'\n",
        "# vocab = load_doc(vocab_filename)\n",
        "# vocab = vocab.split()\n",
        "# vocab = set(vocab)\n",
        "# docs, labels = load_clean_dataset(vocab)\n",
        "# print(len(docs), len(labels))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxOaGGCWIogu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = path + 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode= 'freq' )\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode= 'freq' )\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyuDuTxXNydm",
        "colab_type": "text"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlV0T6FcIokU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model\n",
        "def define_model(n_words):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(50, input_shape=(n_words,), activation= 'relu' ))\n",
        "  model.add(Dense(1, activation= 'sigmoid' ))\n",
        "  model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "  return model\n",
        "\n",
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "  scores = list()\n",
        "  n_repeats = 10\n",
        "  n_words = Xtest.shape[1]\n",
        "  for i in range(n_repeats):\n",
        "    model = define_model(n_words)\n",
        "    model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "    _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "    scores.append(acc)\n",
        "  print( '%d accuracy: %s' % ((i+1), acc))\n",
        "  return scores"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb2hp4izOlOT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "d76a070b-b36e-4e10-c59c-fa8c73207f98"
      },
      "source": [
        "# prepare bag of words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(train_docs)\n",
        "  Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "  Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "  return Xtrain, Xtest\n",
        "\n",
        "\n",
        "\n",
        "vocab_filename = path + 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "train_docs = np.asarray(train_docs)\n",
        "ytrain = np.asarray(ytrain)\n",
        "test_docs = np.asarray(test_docs)\n",
        "ytest = np.asarray(ytest)\n",
        "\n",
        "modes = [ 'binary' , 'count' , 'tfidf' , 'freq' ]\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "  # prepare data for mode\n",
        "  Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "  # evaluate model on data for mode\n",
        "  results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "print(results.describe())\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 accuracy: 0.9399999976158142\n",
            "10 accuracy: 0.9049999713897705\n",
            "10 accuracy: 0.8799999952316284\n",
            "10 accuracy: 0.8799999952316284\n",
            "          binary      count      tfidf       freq\n",
            "count  10.000000  10.000000  10.000000  10.000000\n",
            "mean    0.931000   0.896500   0.871000   0.869000\n",
            "std     0.009369   0.007091   0.019408   0.009068\n",
            "min     0.915000   0.885000   0.835000   0.850000\n",
            "25%     0.925000   0.895000   0.863750   0.865000\n",
            "50%     0.935000   0.895000   0.880000   0.872500\n",
            "75%     0.935000   0.898750   0.883750   0.875000\n",
            "max     0.945000   0.910000   0.890000   0.880000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvQaeFWaaEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "f9c48bfc-867a-40b1-cbe4-6f521b6e347d"
      },
      "source": [
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode= 'binary' )\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode= 'binary' )\n",
        "# define network\n",
        "n_words = Xtrain.shape[1]\n",
        "model = define_model(n_words)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "57/57 - 0s - loss: 0.4817 - accuracy: 0.7794\n",
            "Epoch 2/10\n",
            "57/57 - 0s - loss: 0.0813 - accuracy: 0.9878\n",
            "Epoch 3/10\n",
            "57/57 - 0s - loss: 0.0221 - accuracy: 0.9994\n",
            "Epoch 4/10\n",
            "57/57 - 0s - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "57/57 - 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "57/57 - 0s - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "57/57 - 0s - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "57/57 - 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "57/57 - 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "57/57 - 0s - loss: 0.0011 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f28259c0a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoPwIZeEcOb_",
        "colab_type": "text"
      },
      "source": [
        "## **Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o58adRx0QdNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "28f393ba-926b-4996-ef76-94f74717a8d0"
      },
      "source": [
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "  tokens = clean_doc(review)\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  line = ' ' .join(tokens)\n",
        "  encoded = tokenizer.texts_to_matrix([line], mode= 'binary' )\n",
        "  yhat = model.predict(encoded, verbose=0)\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), ' NEGATIVE '\n",
        "  return percent_pos, ' POSITIVE '\n",
        "\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print( 'Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print( 'Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: [Best movie ever! It was great, I recommend it.]\n",
            "Sentiment:  POSITIVE  (58.316%)\n",
            "Review: [This is a bad movie.]\n",
            "Sentiment:  NEGATIVE  (64.568%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXviR7lvanXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}