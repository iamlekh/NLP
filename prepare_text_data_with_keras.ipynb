{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_text_data_with_keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9xtULoVzytlbuks4qywM7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamlekh/NLP/blob/master/prepare_text_data_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqJ0dd9cp-BM",
        "colab_type": "text"
      },
      "source": [
        "## **Split Words with text to word sequence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOFrL0oBbj7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "523be6e4-178d-4b47-c98c-b7986e830d15"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = ' The quick brown fox jumped over the lazy dog. '\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osT_O8LKqtc5",
        "colab_type": "text"
      },
      "source": [
        "# Encoding with one hot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fv1EJKTqhnk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2e8d7763-b8de-4cf8-bf7a-622bca39ea39"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "[7, 2, 3, 3, 5, 5, 7, 4, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxaXD5i4tdmK",
        "colab_type": "text"
      },
      "source": [
        "## **Hash Encoding with hashing trick**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poTNjz_FrIgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "71ff439e-8a80-41eb-f9b0-e6cd171b6d65"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = ' The quick brown fox jumped over the lazy dog. '\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function= 'md5' )\n",
        "print(result)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqdUhUI6uDZ_",
        "colab_type": "text"
      },
      "source": [
        "## **Tokenizer API**\n",
        "\n",
        "the Tokenizer provides 4 attributes that you can use to query what has been\n",
        "learned about your documents: \n",
        "\n",
        "1.*word counts*: A dictionary of words and their counts.\n",
        "\n",
        "2.*word docs*: An integer count of the total number of documents that were used  to fit the Tokenizer.\n",
        "\n",
        "3.*word index*: A dictionary of words and their uniquely assigned integers.\n",
        "\n",
        "4.*document count*: A dictionary of words and how many documents each appeared in.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*  binary: Whether or not each word is present in the document. This is the default.\n",
        "*  count: The count of each word in the document.\n",
        "*  tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "*  freq: The frequency of each word as a ratio of words within each document.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7C43qQot91V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "9effc29d-a096-41bc-82f1-9c046a4828a9"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = [ ' Well done! ' ,\n",
        "' Good work ' ,\n",
        "' Great effort ' ,\n",
        "' nice work work ' ,\n",
        "' Excellent! ' ]\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "# summarize what was learned\n",
        "print(\"word_counts ---> \",t.word_counts,'\\n')\n",
        "print(\"document_count --->\",t.document_count,'\\n')\n",
        "print(\"word_index --->\",t.word_index,'\\n')\n",
        "print(\"word_docs --->\",t.word_docs,'\\n')\n",
        "\n",
        "encoded_docs = t.texts_to_matrix(docs, mode= 'count' )\n",
        "print(\"count ---> \",encoded_docs,\"\\n\")\n",
        "encoded_docs = t.texts_to_matrix(docs, mode= 'binary' )\n",
        "print(\"binary ---> \",encoded_docs,\"\\n\")\n",
        "encoded_docs = t.texts_to_matrix(docs, mode= 'tfidf' )\n",
        "print(\"tfidf ---> \",encoded_docs,\"\\n\")\n",
        "encoded_docs = t.texts_to_matrix(docs, mode= 'freq' )\n",
        "print(\"frequency ---> \",encoded_docs,\"\\n\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_counts --->  OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 3), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)]) \n",
            "\n",
            "document_count ---> 5 \n",
            "\n",
            "word_index ---> {'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8} \n",
            "\n",
            "word_docs ---> defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'good': 1, 'work': 2, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1}) \n",
            "\n",
            "count --->  [[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 2. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]] \n",
            "\n",
            "binary --->  [[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]] \n",
            "\n",
            "tfidf --->  [[0.         0.         1.25276297 1.25276297 0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.98082925 0.         0.         1.25276297 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         1.25276297\n",
            "  1.25276297 0.         0.        ]\n",
            " [0.         1.66068828 0.         0.         0.         0.\n",
            "  0.         1.25276297 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.25276297]] \n",
            "\n",
            "frequency --->  [[0.         0.         0.5        0.5        0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.5        0.         0.         0.5        0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.5\n",
            "  0.5        0.         0.        ]\n",
            " [0.         0.66666667 0.         0.         0.         0.\n",
            "  0.         0.33333333 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.        ]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}